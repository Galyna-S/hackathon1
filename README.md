
Hackathon1 project Online Retail Transaction Analysis

Hackathon1 project aims to extract valuable insights from online retail transaction data to better understand customer behavior, identify popular products, and optimize pricing and marketing strategies. By analyzing this dataset, businesses can improve their decision-making processes, enhance customer engagement, and boost sales performance.

# ![CI logo](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wdEwwF7XEdOVL9Z8vIqKJg.png)


## Dataset Content
The dataset contains customer transaction data from an online retail platform, including:

Products: Product descriptions and codes.
Transactions: Invoice numbers, dates, and quantities.
Pricing: Unit prices for each product.
Customers: Unique customer IDs and locations.
The dataset is suitable for customer behavior analysis, product popularity insights, and marketing optimization. It is available at Kaggle.

## Business Requirements
The project addresses the following business requirements:

Identify customer purchasing patterns and segments.
Analyze product sales to optimize inventory.
Recommend pricing strategies for higher profitability.
Create customer-focused marketing strategies to improve engagement.


## Hypothesis and how to validate?
Hypotheses:

Most revenue is generated by a small segment of repeat customers (Pareto principle).
Certain products contribute more to revenue due to seasonal trends.
Discounts on popular products will increase overall revenue.

Validation Steps:

Identify high-value customers.
Perform time series analysis to identify seasonal trends.
Testing on discount strategies to measure revenue impact.

## Project Plan
Steps:
Data Collection: Load the dataset from the provided source.
Data Cleaning: Handle missing values, remove duplicates, and standardize formats. 
Data Transformation: Create new features, such as total transaction value.
Analysis: Perform descriptive statistics, and trend analysis.
Visualization: Build interactive dashboards to display insights.

## The rationale to map the business requirements to the Data Visualisations
Product popularity analysis will be conducted and presented using a bar chart showing the top-selling products. This will assist in optimizing inventory and improving product offerings.

Sales trends will be visualized through a time series plot to help identify seasonal trends, allowing businesses to plan their marketing strategies accordingly.

Pricing strategies will be optimized by comparing revenue before and after applying discounts. This analysis will be shown through revenue comparison charts to demonstrate the impact of pricing changes on sales performance.

## Analysis techniques used
Summarize key metrics like total sales, average transaction values, and customer count.
Identify sales trends and seasonality over time.

Challenges and Solutions:

Missing values were handled using imputation techniques.
Categorical variables were encoded for analysis.
Seasonal patterns were extracted using moving averages and decomposition methods.

Use of AI Tools:

ChatGPT was used for brainstorming ideas and optimizing Python code.

## Ethical considerations
Data Privacy: Customer IDs were anonymized to protect personal data.
Bias and Fairness: The dataset was checked for sampling bias to ensure accurate analysis.
Legal Compliance: The data was used within the permitted scope of the Kaggle license.

## Dashboard Design
Dashboard Pages:

Home Page: Overview of key insights and metrics.
Sales Trends: Interactive time series plots.
Product Analysis: Bar charts of top-selling products and revenue contributions.

Design Strategy:

The dashboard was designed with both technical and non-technical audiences in mind.
Simplified plots and tooltips were added for clarity.
The layout was optimized for easy navigation and interactivity.


## Unfixed Bugs
Duplicate Transactions: There were a few duplicated transaction records that were not removed during data cleaning due to time constraints.

Geolocation Data: The dataset lacks complete geolocation information, limiting regional analysis.
Slug Size Issue on Heroku: The dataset size exceeded Heroku's slug size limit, requiring a workaround using .slugignore.

Gaps and Solutions:

Identified gaps in time series forecasting, addressed through additional research and tutorials.
Feedback from peers was used to improve dashboard visuals and code readability.


## Development Roadmap
Challenges Faced:

Handling large datasets without performance degradation.
Optimizing the ETL pipeline for faster data processing.
Visualizing complex relationships between variables.

Next Steps:

Implement machine learning models for customer churn prediction.
Explore cloud-based solutions for scalable data storage and analysis.

## Deployment
### Heroku

* The App live link is: https://YOUR_APP_NAME.herokuapp.com/ 
* Set the runtime.txt Python version to a [Heroku-20](https://devcenter.heroku.com/articles/python-support#supported-runtimes) stack currently supported version.
* The project was deployed to Heroku using the following steps.

1. Log in to Heroku and create an App
2. From the Deploy tab, select GitHub as the deployment method.
3. Select your repository name and click Search. Once it is found, click Connect.
4. Select the branch you want to deploy, then click Deploy Branch.
5. The deployment process should happen smoothly if all deployment files are fully functional. Click now the button Open App on the top of the page to access your App.
6. If the slug size is too large then add large files not required for the app to the .slugignore file.


## Main Data Analysis Libraries
Pandas: For data cleaning and manipulation.
NumPy: For numerical computations.
Matplotlib / Seaborn: For visualizations.

## Credits 

Hackathon techniques were inspired by the Code Institute tutorial.
Visualization styles were based on the Seaborn documentation.

### Content 

- The text for analysis was taken from Wikipedia [Pareto Principle](https://en.wikipedia.org/wiki/Pareto_principle)

### Media

The photos used on the home and sign-up page are from Medium.com

## Acknowledgements (optional)
I would like to thank my wmca-data-analytics team for their invaluable feedback and support throughout this project. Special thanks to Code Institute for providing the resources and guidance necessary for completing this project.